{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文数据集\n",
    "本文采用了清华NLP组提供的THUCNews新闻文本分类数据集的一个子集（原始的数据集大约74万篇文档，训练起来需要花较长的时间）。数据集请自行到THUCTC：一个高效的中文文本分类工具包下载，请遵循数据提供方的开源协议。\n",
    "\n",
    "本次训练使用了其中的10个分类，每个分类6500条，总共65000条新闻数据。\n",
    "\n",
    "包括：体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "read_file(): 读取文件数据;\n",
    "build_vocab(): 构建词汇表，使用字符级的表示，这一函数会将词汇表存储下来，避免每一次重复处理;\n",
    "read_vocab(): 读取上一步存储的词汇表，转换为{词：id}表示;\n",
    "read_category(): 将分类目录固定，转换为{类别: id}表示;\n",
    "to_words(): 将一条由id表示的数据重新转换为文字;\n",
    "preocess_file(): 将数据集从文字转换为固定长度的id序列表示;\n",
    "batch_iter(): 为神经网络的训练准备经过shuffle的批次的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as kr\n",
    "\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [native_content(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
